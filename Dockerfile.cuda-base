# Dockerfile.cuda-base — CUDA 12.4 base image (heavy layer, rarely rebuilt)
#
# Built ON TOP of pytorch/pytorch:2.6.0-cuda12.4-cudnn9-runtime from Docker Hub.
# That base image (~3 GB) is widely cached on GPU-cloud hosts so only our thin
# custom layer needs to be pulled from GHCR on a cold machine (~1 GB vs ~4 GB).
#
# Contains all Python + CUDA + ML dependencies but NO app source code.
# This image is published as ghcr.io/coriou/voice-tools:cuda-base and changes
# only when pyproject.toml / poetry.lock / flash-attn change.
#
# Build & push:
#   make publish-cuda-base               (first time, or on dep upgrades)
#
# Then build the thin app image on top:
#   make publish-cuda                    (every code change — pulls ~50 MB)

FROM pytorch/pytorch:2.6.0-cuda12.4-cudnn9-runtime

LABEL org.opencontainers.image.source="https://github.com/Coriou/py-audio-box"
LABEL org.opencontainers.image.description="ML/audio toolbox — CUDA 12.4 base (deps only, no app src)"
LABEL org.opencontainers.image.licenses="MIT"

ARG POETRY_VERSION=1.8.3

ENV \
    PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    PIP_DISABLE_PIP_VERSION_CHECK=1 \
    PIP_NO_CACHE_DIR=1 \
    POETRY_NO_INTERACTION=1 \
    POETRY_VIRTUALENVS_CREATE=false \
    POETRY_CACHE_DIR=/root/.cache/pypoetry

# --- OS deps ---
RUN apt-get update && apt-get install -y --no-install-recommends \
    ca-certificates \
    curl \
    git \
    rsync \
    ffmpeg \
    sox \
    coreutils \
    tini \
    jq \
    nodejs \
    vim \
    htop \
    && rm -rf /var/lib/apt/lists/*

# Install Poetry into conda Python (already on PATH as /opt/conda/bin/poetry)
RUN pip install "poetry==${POETRY_VERSION}" && poetry --version

# Copy only dependency manifests (for layer caching)
COPY pyproject.toml poetry.lock* /app/
WORKDIR /app

# Install Python deps into conda site-packages.
# torch, torchaudio, torchcodec are all torch-version-coupled and installed
# explicitly below.  Filter them here AND use --no-deps so that pip never
# resolves/installs a CPU-variant torch while satisfying transitive deps.
# This is safe because poetry.lock is a complete, already-resolved closure.
RUN --mount=type=cache,target=/root/.cache/pypoetry \
    --mount=type=cache,target=/root/.cache/pip \
    poetry export --without-hashes -f requirements.txt \
    | grep -Ev "^(torch|torchaudio|torchcodec)[^a-zA-Z]" \
    > /tmp/reqs.txt \
    && pip install --no-deps -r /tmp/reqs.txt

# ── Replace conda-torch with pip-torch to get a consistent cxx11 ABI ──────────
#
# pytorch/pytorch:* Docker images ship torch installed via conda.  Conda-built
# torch uses _GLIBCXX_USE_CXX11_ABI=0 ("cxx11abiFALSE" / old ABI), while all
# pip-distributed wheels from download.pytorch.org use the NEW ABI
# (_GLIBCXX_USE_CXX11_ABI=1, "cxx11abiTRUE").
#
# flash-attn pre-built wheels on GitHub come in two variants:
#   cxx11abiFALSE → links against old-ABI torch symbols (conda)
#   cxx11abiTRUE  → links against new-ABI torch symbols (pip)
#
# If the ABI doesn't match, flash_attn.so fails on import with
# "undefined symbol: _ZN3c10..." → ImportError → silent fallback to SDPA →
# 0 % GPU utilisation and ~1× real-time factor on any GPU.
#
# FIX: force-reinstall torch AND torchaudio from the pip cu124 index in the
# same layer.  This overwrites the conda-installed torch.so files in
# /opt/conda/lib/python3.11/site-packages/torch/ with pip variants, making
# every C++ extension in this image use the same (new) ABI.
# The cxx11abiTRUE flash-attn wheel then loads cleanly.
ARG TORCH_VERSION=2.6.0
ARG TORCHCODEC_VERSION=0.6.0
RUN --mount=type=cache,target=/root/.cache/pip \
    pip install --force-reinstall \
    "torch==${TORCH_VERSION}" \
    "torchaudio==${TORCH_VERSION}" \
    --index-url "https://download.pytorch.org/whl/cu124" \
    && pip install "torchcodec==${TORCHCODEC_VERSION}"

# ── flash-attn pre-built wheel (avoids ~20 min compile, must match torch + CUDA) ──
# Uses cxx11abiTRUE because we just installed pip-torch above (new ABI).
# If you ever switch back to conda-torch, use the cxx11abiFALSE variant instead.
ARG FLASH_ATTN_WHEEL=https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.3/flash_attn-2.8.3+cu12torch2.6cxx11abiTRUE-cp311-cp311-linux_x86_64.whl
RUN --mount=type=cache,target=/root/.cache/pip \
    pip install --no-build-isolation "${FLASH_ATTN_WHEEL}"

# Install vastai CLI
RUN --mount=type=cache,target=/root/.cache/pip \
    pip install vastai

# Persistent-volume mount targets + disable vast.ai auto-tmux
RUN mkdir -p /work /cache && touch /root/.no_auto_tmux

# Runtime environment — all paths point to /cache so model weights land in the
# persistent volume when one is attached at /cache.
ENV XDG_CACHE_HOME=/cache \
    HF_HOME=/cache/huggingface \
    TORCH_HOME=/cache/torch \
    TORCH_DEVICE=cuda \
    HF_HUB_DISABLE_XET_TRANSPORT=1 \
    TORCH_COMPUTE=cu124

WORKDIR /app
ENTRYPOINT ["/usr/bin/tini", "--"]
CMD ["bash"]
