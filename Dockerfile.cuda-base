# Dockerfile.cuda-base — CUDA 12.4 base image (heavy layer, rarely rebuilt)
#
# Built ON TOP of pytorch/pytorch:2.6.0-cuda12.4-cudnn9-runtime from Docker Hub.
# That base image (~3 GB) is widely cached on GPU-cloud hosts so only our thin
# custom layer needs to be pulled from GHCR on a cold machine (~1 GB vs ~4 GB).
#
# Contains all Python + CUDA + ML dependencies but NO app source code.
# This image is published as ghcr.io/coriou/voice-tools:cuda-base and changes
# only when pyproject.toml / poetry.lock / flash-attn change.
#
# Build & push:
#   make publish-cuda-base               (first time, or on dep upgrades)
#
# Then build the thin app image on top:
#   make publish-cuda                    (every code change — pulls ~50 MB)

FROM pytorch/pytorch:2.6.0-cuda12.4-cudnn9-runtime

LABEL org.opencontainers.image.source="https://github.com/Coriou/py-audio-box"
LABEL org.opencontainers.image.description="ML/audio toolbox — CUDA 12.4 base (deps only, no app src)"
LABEL org.opencontainers.image.licenses="MIT"

ARG POETRY_VERSION=1.8.3

ENV \
  PYTHONDONTWRITEBYTECODE=1 \
  PYTHONUNBUFFERED=1 \
  PIP_DISABLE_PIP_VERSION_CHECK=1 \
  PIP_NO_CACHE_DIR=1 \
  POETRY_NO_INTERACTION=1 \
  POETRY_VIRTUALENVS_CREATE=false \
  POETRY_CACHE_DIR=/root/.cache/pypoetry

# --- OS deps ---
RUN apt-get update && apt-get install -y --no-install-recommends \
  ca-certificates \
  curl \
  git \
  rsync \
  ffmpeg \
  sox \
  coreutils \
  tini \
  jq \
  nodejs \
  vim \
  htop \
  && rm -rf /var/lib/apt/lists/*

# Install Poetry into conda Python (already on PATH as /opt/conda/bin/poetry)
RUN pip install "poetry==${POETRY_VERSION}" && poetry --version

# Copy only dependency manifests (for layer caching)
COPY pyproject.toml poetry.lock* /app/
WORKDIR /app

# Install Python deps into conda site-packages.
# torch, torchaudio, torchcodec are all torch-version-coupled and installed
# explicitly below.  Filter them here so pip does not pull in the CPU wheel
# variants when resolving transitive dependencies.
RUN --mount=type=cache,target=/root/.cache/pypoetry \
  --mount=type=cache,target=/root/.cache/pip \
  poetry export --without-hashes -f requirements.txt \
    | grep -Ev "^(torch|torchaudio|torchcodec)[^a-zA-Z]" \
    > /tmp/reqs.txt \
  && pip install -r /tmp/reqs.txt

# Install torch-coupled packages from the CUDA 12.4 wheel index / PyPI.
# torchcodec version must match the major.minor of torch (0.6.x ↔ torch 2.6.x).
ARG TORCH_VERSION=2.6.0
ARG TORCHCODEC_VERSION=0.6.0
RUN --mount=type=cache,target=/root/.cache/pip \
  pip install "torchaudio==${TORCH_VERSION}" \
  --index-url "https://download.pytorch.org/whl/cu124" \
  && pip install "torchcodec==${TORCHCODEC_VERSION}"

# ── flash-attn pre-built wheel (avoids ~20 min compile, must match torch + CUDA) ──
ARG FLASH_ATTN_WHEEL=https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.3/flash_attn-2.8.3+cu12torch2.6cxx11abiTRUE-cp311-cp311-linux_x86_64.whl
RUN --mount=type=cache,target=/root/.cache/pip \
  pip install --no-build-isolation "${FLASH_ATTN_WHEEL}"

# Install vastai CLI
RUN --mount=type=cache,target=/root/.cache/pip \
  pip install vastai

# Persistent-volume mount targets + disable vast.ai auto-tmux
RUN mkdir -p /work /cache && touch /root/.no_auto_tmux

# Runtime environment — all paths point to /cache so model weights land in the
# persistent volume when one is attached at /cache.
ENV XDG_CACHE_HOME=/cache \
  HF_HOME=/cache/huggingface \
  TORCH_HOME=/cache/torch \
  TORCH_DEVICE=cuda \
  HF_HUB_DISABLE_XET_TRANSPORT=1 \
  TORCH_COMPUTE=cu124

WORKDIR /app
ENTRYPOINT ["/usr/bin/tini", "--"]
CMD ["bash"]
